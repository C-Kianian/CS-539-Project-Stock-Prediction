{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2e9e1-7841-4f69-9bb8-c6a6f78392ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the imports\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f34571-81d1-4c09-abdf-782a60efefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and originze data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f2c1c-8d1f-4352-9bce-4623c58c5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the Model \n",
    "\n",
    "#initialize weights \n",
    "def _init_weights(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')  # He init\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#overall model\n",
    "class StockMovementMLP(nn.Module):\n",
    "    def __init__(self, T: int):\n",
    "        super().__init__()\n",
    "        input_dim = 20 * T\n",
    "        hidden_dims = [256, 128, 64, 32]  # try a different one [256,512,64,32]\n",
    "        layers = []\n",
    "\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(dims)-1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            #--------------------------#\n",
    "            layers.append(nn.Dropout(0.2))  # Regularization make another model without this and see how it does\n",
    "            #==========================#\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 7)  # one output per time horizon\n",
    "\n",
    "        self._init_weights() # initialize weights here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return self.output_layer(x)  # [batch_size, 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3f4b8-52c6-4c2a-a819-bf183346b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=1e-3, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                val_pred = model(X_val)\n",
    "                val_loss += criterion(val_pred, y_val).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(model.state_dict(), f'checkpoint_epoch{epoch}.pth')\n",
    "\n",
    "        # Print stats\n",
    "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Plot losses\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training vs Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Ask user if they want to stop\n",
    "        stop = input(\"Type 'stop' to end training early, or press Enter to continue: \")\n",
    "        if stop.strip().lower() == 'stop':\n",
    "            print(\"Stopping early by user request.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3a508-3ae7-41f6-b6bd-80ebaf2a20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c9c96-c401-4992-9f81-2bc8650b3dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
